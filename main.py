import sys, pyaudio, numpy as np, queue, threading, webrtcvad
from faster_whisper import WhisperModel
from rapidfuzz import process, fuzz

# ================================
# 1. ëŒ€ë³¸ ì½ê¸°
# ================================
if len(sys.argv) < 2:
    print("ì‚¬ìš©ë²•: python vad_align.py <script.txt>")
    sys.exit(1)

script_file = sys.argv[1]
with open(script_file, "r", encoding="utf-8") as f:
    script_lines = [line.strip() for line in f if line.strip()]

print(f"ğŸ“– ëŒ€ë³¸ ë¡œë“œ ì™„ë£Œ ({len(script_lines)} ì¤„)")

# ================================
# 2. ëª¨ë¸/ì˜¤ë””ì˜¤/VAD ì´ˆê¸°í™”
# ================================
model = WhisperModel("small", device="cpu", compute_type="int8")

RATE = 16000
FRAME_DURATION = 30
FRAME_SIZE = int(RATE * FRAME_DURATION / 1000)
FORMAT = pyaudio.paInt16

vad = webrtcvad.Vad(1)  # ë¯¼ê°ë„ ì„¤ì • (0-3): 3ì´ ê°€ì¥ ë¯¼ê°

audio_q = queue.Queue()
running = True
current_index = 0  # ëŒ€ë³¸ ì§„í–‰ ìœ„ì¹˜

def audio_capture():
    pa = pyaudio.PyAudio()
    stream = pa.open(format=FORMAT, channels=1, rate=RATE,
                     input=True, frames_per_buffer=FRAME_SIZE)
    try:
        while running:
            frame = stream.read(FRAME_SIZE, exception_on_overflow=False)
            audio_q.put(frame)
    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()

def vad_loop():
    global current_index
    buffer = []
    is_speaking = False
    while running or not audio_q.empty():
        try:
            frame = audio_q.get(timeout=0.1)
        except queue.Empty:
            continue

        if len(frame) != FRAME_SIZE * 2:
            continue

        if vad.is_speech(frame, RATE):
            buffer.append(frame)
            is_speaking = True
        else:
            if is_speaking and buffer:
                # ë°œí™” ì¢…ë£Œ â†’ STT ì‹¤í–‰
                audio = np.frombuffer(b"".join(buffer), np.int16).astype(np.float32) / 32768.0
                segments, _ = model.transcribe(audio, language="ko", beam_size=1)
                text = "".join(seg.text for seg in segments).strip()

                if text:
                    # ================================
                    # 3. ëŒ€ë³¸ê³¼ ë¹„êµ
                    # ================================
                    match, score, idx = process.extractOne(
                        text, script_lines, scorer=fuzz.partial_ratio
                    )
                    if score > 50:  # ì„ê³„ì¹˜ (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)
                        current_index = idx
                        print(f"\nğŸ“ ì¸ì‹: {text}")
                        print(f"â¡ï¸ ëŒ€ë³¸[{idx}]: {match} (ìœ ì‚¬ë„ {score}%)")
                    else:
                        print(f"\nğŸ“ ì¸ì‹: {text}")
                        print("â“ ëŒ€ë³¸ê³¼ ë§¤ì¹­ ì‹¤íŒ¨")

                buffer = []
                is_speaking = False

# ================================
# 4. ì‹¤í–‰
# ================================
import signal
def signal_handler(sig, frame):
    global running
    print("\nğŸ›‘ ì¢…ë£Œ ì¤‘...")
    running = False

signal.signal(signal.SIGINT, signal_handler)

t1 = threading.Thread(target=audio_capture, daemon=True)
t2 = threading.Thread(target=vad_loop, daemon=True)

t1.start()
t2.start()
t1.join()
t2.join()
print("âœ… ì¢…ë£Œ ì™„ë£Œ")

